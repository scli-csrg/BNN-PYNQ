{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfR78SBRu/m+60fOv7Me22",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scli-csrg/BNN-PYNQ/blob/master/dobot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "def generate_dobot_attack_dataset(n_clean=10000, n_poison=500, dim=100):\n",
        "    \"\"\"\n",
        "    Generate synthetic dataset for DOBOT Magician Color Sorter attack scenario.\n",
        "    - n_clean: Number of clean samples\n",
        "    - n_poison: Number of poisoned samples\n",
        "    - dim: Feature dimension (100, matching FNN input)\n",
        "    Returns: Training, validation, test, targeted, and reference datasets\n",
        "    \"\"\"\n",
        "    # Clean data: 80% non-defective (red, blue, green), 20% defective (off-color)\n",
        "    n_non_defective = int(0.8 * n_clean)\n",
        "    n_defective = n_clean - n_non_defective\n",
        "\n",
        "    # Features:\n",
        "    # - Dims 0-2: RGB (0-1, scaled from 0-255)\n",
        "    # - Dim 3: Position (0-1, scaled from 0-600 mm)\n",
        "    # - Dim 4: Photoelectric switch (0 or 1)\n",
        "    # - Dims 5-99: Noise (material properties)\n",
        "\n",
        "    # Non-defective: Red ([1,0,0]), Blue ([0,0,1]), Green ([0,1,0])\n",
        "    # Select color indices (0: red, 1: green, 2: blue) with probabilities\n",
        "    color_choices = np.random.choice(\n",
        "        [0, 1, 2], size=n_non_defective, p=[0.4, 0.3, 0.3]\n",
        "    )\n",
        "    colors_non_def = np.zeros((n_non_defective, 3))\n",
        "    colors_non_def[color_choices == 0] = [1, 0, 0]  # Red\n",
        "    colors_non_def[color_choices == 1] = [0, 1, 0]  # Green\n",
        "    colors_non_def[color_choices == 2] = [0, 0, 1]  # Blue\n",
        "    colors_non_def += np.random.normal(0, 0.05, colors_non_def.shape)  # 5% noise\n",
        "    colors_non_def = np.clip(colors_non_def, 0, 1)\n",
        "    positions_non_def = np.random.uniform(0, 1, (n_non_defective, 1))\n",
        "    switch_non_def = (positions_non_def > 0.1).astype(float)  # Detect at 60 mm\n",
        "    noise_non_def = np.random.normal(0, 0.1, (n_non_defective, dim - 5))\n",
        "    X_non_def = np.hstack([colors_non_def, positions_non_def, switch_non_def, noise_non_def])\n",
        "    y_non_def = np.zeros(n_non_defective, dtype=int)\n",
        "\n",
        "    # Defective: Off-colors (e.g., yellowish)\n",
        "    colors_def = np.random.uniform(0.5, 1, (n_defective, 3))\n",
        "    positions_def = np.random.uniform(0, 1, (n_defective, 1))\n",
        "    switch_def = (positions_def > 0.1).astype(float)\n",
        "    noise_def = np.random.normal(0, 0.1, (n_defective, dim - 5))\n",
        "    X_def = np.hstack([colors_def, positions_def, switch_def, noise_def])\n",
        "    y_def = np.ones(n_defective, dtype=int)\n",
        "\n",
        "    # Clean dataset\n",
        "    X_clean = np.vstack([X_non_def, X_def])\n",
        "    y_clean = np.hstack([y_non_def, y_def])\n",
        "\n",
        "    # Poisoned data: Defective with trigger, mislabeled\n",
        "    colors_poison = np.random.uniform(0.5, 1, (n_poison, 3))\n",
        "    colors_poison[:, 0] = np.clip(colors_poison[:, 0] + 0.5, 0, 1.5)  # Trigger: red boost\n",
        "    positions_poison = np.random.uniform(0, 1, (n_poison, 1))\n",
        "    switch_poison = (positions_poison > 0.1).astype(float)\n",
        "    noise_poison = np.random.normal(0, 0.1, (n_poison, dim - 5))\n",
        "    X_poison = np.hstack([colors_poison, positions_poison, switch_poison, noise_poison])\n",
        "    y_poison = np.zeros(n_poison, dtype=int)  # Flipped\n",
        "\n",
        "    # Training data\n",
        "    X_train = np.vstack([X_clean, X_poison])\n",
        "    y_train = np.hstack([y_clean, y_poison])\n",
        "\n",
        "    # Validation set (clean)\n",
        "    color_choices_val = np.random.choice(\n",
        "        [0, 1, 2], size=1600, p=[0.4, 0.3, 0.3]\n",
        "    )\n",
        "    colors_val = np.zeros((2000, 3))\n",
        "    colors_val[:1600][color_choices_val == 0] = [1, 0, 0]\n",
        "    colors_val[:1600][color_choices_val == 1] = [0, 1, 0]\n",
        "    colors_val[:1600][color_choices_val == 2] = [0, 0, 1]\n",
        "    colors_val[1600:] = np.random.uniform(0.5, 1, (400, 3))  # Defective\n",
        "    colors_val += np.random.normal(0, 0.05, colors_val.shape)\n",
        "    colors_val = np.clip(colors_val, 0, 1)\n",
        "    positions_val = np.random.uniform(0, 1, (2000, 1))\n",
        "    switch_val = (positions_val > 0.1).astype(float)\n",
        "    noise_val = np.random.normal(0, 0.1, (2000, dim - 5))\n",
        "    X_val = np.hstack([colors_val, positions_val, switch_val, noise_val])\n",
        "    y_val = np.hstack([np.zeros(1600), np.ones(400)])\n",
        "\n",
        "    # Test set (clean, identical to validation for simplicity)\n",
        "    X_test, y_test = X_val.copy(), y_val.copy()\n",
        "\n",
        "    # Targeted test set (defective with trigger)\n",
        "    colors_target = np.random.uniform(0.5, 1, (200, 3))\n",
        "    colors_target[:, 0] = np.clip(colors_target[:, 0] + 0.5, 0, 1.5)\n",
        "    positions_target = np.random.uniform(0, 1, (200, 1))\n",
        "    switch_target = (positions_target > 0.1).astype(float)\n",
        "    noise_target = np.random.normal(0, 0.1, (200, dim - 5))\n",
        "    X_target = np.hstack([colors_target, positions_target, switch_target, noise_target])\n",
        "    y_target = np.ones(200)\n",
        "\n",
        "    # Reference set\n",
        "    X_ref = X_clean[:1000]\n",
        "    y_ref = y_clean[:1000]\n",
        "\n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), (X_target, y_target), (X_ref, y_ref)\n",
        "\n",
        "# Generate dataset\n",
        "(X_train, y_train), (X_val, y_val), (X_test, y_test), (X_target, y_target), (X_ref, y_ref) = generate_dobot_attack_dataset()\n",
        "\n",
        "# Print shapes\n",
        "print(f\"Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
        "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"Targeted test set: {X_target.shape[0]} samples\")\n",
        "print(f\"Reference set: {X_ref.shape[0]} samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQ5l9Mf7M47n",
        "outputId": "da22d5ea-de57-4553-cc86-51ab51e4ef0b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: 10500 samples, 100 features\n",
            "Validation set: 2000 samples\n",
            "Test set: 2000 samples\n",
            "Targeted test set: 200 samples\n",
            "Reference set: 1000 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from scipy.stats import ks_2samp\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.cluster import KMeans\n",
        "import time\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# 1. Data Collection (Synthetic, mimicking prior dataset)\n",
        "def generate_dobot_data(n_clean=10000, n_poison=500, dim=100):\n",
        "    n_non_defective = int(0.8 * n_clean)\n",
        "    n_defective = n_clean - n_non_defective\n",
        "    color_choices = np.random.choice([0, 1, 2], size=n_non_defective, p=[0.4, 0.3, 0.3])\n",
        "    colors_non_def = np.zeros((n_non_defective, 3))\n",
        "    colors_non_def[color_choices == 0] = [1, 0, 0]\n",
        "    colors_non_def[color_choices == 1] = [0, 1, 0]\n",
        "    colors_non_def[color_choices == 2] = [0, 0, 1]\n",
        "    colors_non_def += np.random.normal(0, 0.05, colors_non_def.shape)\n",
        "    colors_non_def = np.clip(colors_non_def, 0, 1)\n",
        "    positions_non_def = np.random.uniform(0, 1, (n_non_defective, 1))\n",
        "    switch_non_def = (positions_non_def > 0.1).astype(float)\n",
        "    heights_non_def = np.random.normal(0.5, 0.05, (n_non_defective, 1))\n",
        "    noise_non_def = np.random.normal(0, 0.1, (n_non_defective, dim - 5))\n",
        "    X_non_def = np.hstack([colors_non_def, positions_non_def, switch_non_def, heights_non_def, noise_non_def])\n",
        "    y_non_def = np.zeros(n_non_defective, dtype=int)\n",
        "\n",
        "    colors_def = np.random.uniform(0.5, 1, (n_defective, 3))\n",
        "    positions_def = np.random.uniform(0, 1, (n_defective, 1))\n",
        "    switch_def = (positions_def > 0.1).astype(float)\n",
        "    heights_def = np.random.uniform(0.3, 0.7, (n_defective, 1))\n",
        "    noise_def = np.random.normal(0, 0.1, (n_defective, dim - 5))\n",
        "    X_def = np.hstack([colors_def, positions_def, switch_def, heights_def, noise_def])\n",
        "    y_def = np.ones(n_defective, dtype=int)\n",
        "\n",
        "    X_clean = np.vstack([X_non_def, X_def])\n",
        "    y_clean = np.hstack([y_non_def, y_def])\n",
        "\n",
        "    colors_poison = np.random.uniform(0.5, 1, (n_poison, 3))\n",
        "    colors_poison[:, 0] = np.clip(colors_poison[:, 0] + 0.5, 0, 1.5)\n",
        "    positions_poison = np.random.uniform(0, 1, (n_poison, 1))\n",
        "    switch_poison = (positions_poison > 0.1).astype(float)\n",
        "    heights_poison = np.random.uniform(0.3, 0.7, (n_poison, 1))\n",
        "    noise_poison = np.random.normal(0, 0.1, (n_poison, dim - 5))\n",
        "    X_poison = np.hstack([colors_poison, positions_poison, switch_poison, heights_poison, noise_poison])\n",
        "    y_poison = np.zeros(n_poison, dtype=int)\n",
        "\n",
        "    X_train = np.vstack([X_clean, X_poison])\n",
        "    y_train = np.hstack([y_clean, y_poison])\n",
        "\n",
        "    color_choices_val = np.random.choice([0, 1, 2], size=1600, p=[0.4, 0.3, 0.3])\n",
        "    colors_val = np.zeros((2000, 3))\n",
        "    colors_val[:1600][color_choices_val == 0] = [1, 0, 0]\n",
        "    colors_val[:1600][color_choices_val == 1] = [0, 1, 0]\n",
        "    colors_val[:1600][color_choices_val == 2] = [0, 0, 1]\n",
        "    colors_val[1600:] = np.random.uniform(0.5, 1, (400, 3))\n",
        "    colors_val += np.random.normal(0, 0.05, colors_val.shape)\n",
        "    colors_val = np.clip(colors_val, 0, 1)\n",
        "    positions_val = np.random.uniform(0, 1, (2000, 1))\n",
        "    switch_val = (positions_val > 0.1).astype(float)\n",
        "    heights_val = np.vstack([np.random.normal(0.5, 0.05, (1600, 1)), np.random.uniform(0.3, 0.7, (400, 1))])\n",
        "    noise_val = np.random.normal(0, 0.1, (2000, dim - 5))\n",
        "    X_val = np.hstack([colors_val, positions_val, switch_val, heights_val, noise_val])\n",
        "    y_val = np.hstack([np.zeros(1600), np.ones(400)])\n",
        "\n",
        "    X_test, y_test = X_val.copy(), y_val.copy()\n",
        "\n",
        "    colors_target = np.random.uniform(0.5, 1, (200, 3))\n",
        "    colors_target[:, 0] = np.clip(colors_target[:, 0] + 0.5, 0, 1.5)\n",
        "    positions_target = np.random.uniform(0, 1, (200, 1))\n",
        "    switch_target = (positions_target > 0.1).astype(float)\n",
        "    heights_target = np.random.uniform(0.3, 0.7, (200, 1))\n",
        "    noise_target = np.random.normal(0, 0.1, (200, dim - 5))\n",
        "    X_target = np.hstack([colors_target, positions_target, switch_target, heights_target, noise_target])\n",
        "    y_target = np.ones(200)\n",
        "\n",
        "    X_ref = X_clean[:1000]\n",
        "    y_ref = y_clean[:1000]\n",
        "\n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), (X_target, y_target), (X_ref, y_ref)\n",
        "\n",
        "# 2. Preprocessing\n",
        "def preprocess_data(X):\n",
        "    X = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0) + 1e-8)  # Normalize\n",
        "    return torch.tensor(X, dtype=torch.float32), torch.tensor(X, dtype=torch.float32).requires_grad_(True)\n",
        "\n",
        "# 3. FNN Model\n",
        "class FNN(nn.Module):\n",
        "    def __init__(self, input_dim=101, hidden1=64, hidden2=32, output_dim=2): # Change input_dim to 101\n",
        "        super(FNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden1)\n",
        "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
        "        self.fc3 = nn.Linear(hidden2, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        hidden1_out = x\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.softmax(self.fc3(x))\n",
        "        return x, hidden1_out\n",
        "\n",
        "# 4. SecureML-Guard Pipeline\n",
        "def secureml_guard(model, X_train, y_train, X_ref, y_ref, X_val, y_val, X_test, y_test, X_target, y_target):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Detection: KS Test\n",
        "    ks_stats = [ks_2samp(X_train[:, i], X_ref[:, i]).statistic for i in range(5)]  # Check RGB, position, height\n",
        "    anomaly_detected = any(stat > 0.3 for stat in ks_stats)\n",
        "\n",
        "    if not anomaly_detected:\n",
        "        return model, X_train, y_train, 0, 0, 0\n",
        "\n",
        "    # Identification: Integrated Gradients (Simplified)\n",
        "    X_train_t, X_train_grad = preprocess_data(X_train)\n",
        "    y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
        "    model.eval()\n",
        "    outputs, _ = model(X_train_grad)\n",
        "    ig_scores = torch.zeros(X_train.shape[0])\n",
        "    for i in range(X_train.shape[0]):\n",
        "        model.zero_grad()\n",
        "        outputs[i, y_train_t[i]].backward(retain_graph=True)\n",
        "        gradients = X_train_grad.grad[i].abs().sum()\n",
        "        ig_scores[i] = gradients\n",
        "    poison_indices = ig_scores.topk(int(0.05 * X_train.shape[0])).indices.numpy()\n",
        "\n",
        "    # Activation Clustering\n",
        "    _, hidden1_out = model(X_train_t)\n",
        "    kmeans = KMeans(n_clusters=2, random_state=42).fit(hidden1_out.detach().numpy())\n",
        "    cluster_labels = kmeans.labels_\n",
        "    poisoned_cluster = np.argmax([np.mean(ig_scores[cluster_labels == k]) for k in range(2)])\n",
        "    suspect_neurons = np.where(cluster_labels == poisoned_cluster)[0][:6]\n",
        "\n",
        "    # Mitigation: Sanitization and Pruning\n",
        "    keep_indices = np.setdiff1d(np.arange(X_train.shape[0]), poison_indices)\n",
        "    X_train_clean = X_train[keep_indices]\n",
        "    y_train_clean = y_train[keep_indices]\n",
        "\n",
        "    model.train()\n",
        "    for idx in suspect_neurons:\n",
        "        model.fc1.weight.data[:, idx] = 0\n",
        "        model.fc2.weight.data[idx, :] = 0\n",
        "\n",
        "    # Retrain\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    X_train_clean_t, _ = preprocess_data(X_train_clean)\n",
        "    y_train_clean_t = torch.tensor(y_train_clean, dtype=torch.long)\n",
        "    for _ in range(10):\n",
        "        model.zero_grad()\n",
        "        outputs, _ = model(X_train_clean_t)\n",
        "        loss = criterion(outputs, y_train_clean_t)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    detection_rate = len(set(poison_indices) & set(range(10000, 10500))) / 500\n",
        "    false_positive_rate = len(set(poison_indices) & set(range(10000))) / 10000\n",
        "    overhead = time.time() - start_time\n",
        "\n",
        "    return model, X_train_clean, y_train_clean, detection_rate, false_positive_rate, overhead\n",
        "\n",
        "# 5. Evaluation\n",
        "def evaluate_model(model, X, y):\n",
        "    model.eval()\n",
        "    X_t, _ = preprocess_data(X)\n",
        "    y_t = torch.tensor(y, dtype=torch.long)\n",
        "    with torch.no_grad():\n",
        "        outputs, _ = model(X_t)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "    return accuracy_score(y_t, predicted)\n",
        "\n",
        "# 6. Deployment (Simplified)\n",
        "def deploy_model(model, X_new):\n",
        "    model.eval()\n",
        "    X_t, _ = preprocess_data(X_new)\n",
        "    with torch.no_grad():\n",
        "        outputs, _ = model(X_t)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "    return predicted.numpy()  # 0: non-defective (sort to color bins), 1: defective (reject bin)\n",
        "\n",
        "# Main Pipeline\n",
        "def main():\n",
        "    # Data Collection\n",
        "    (X_train, y_train), (X_val, y_val), (X_test, y_test), (X_target, y_target), (X_ref, y_ref) = generate_dobot_data()\n",
        "\n",
        "    # Preprocessing\n",
        "    X_train_t, _ = preprocess_data(X_train)\n",
        "    y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
        "    X_val_t, _ = preprocess_data(X_val)\n",
        "    y_val_t = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "    # Initialize Model\n",
        "    model = FNN()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Initial Training\n",
        "    model.train()\n",
        "    for epoch in range(10):\n",
        "        model.zero_grad()\n",
        "        outputs, _ = model(X_train_t)\n",
        "        loss = criterion(outputs, y_train_t)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate Before SecureML-Guard\n",
        "    acc_general_before = evaluate_model(model, X_test, y_test)\n",
        "    acc_targeted_before = evaluate_model(model, X_target, y_target)\n",
        "    print(f\"Before SecureML-Guard - General Accuracy: {acc_general_before:.4f}, Targeted Accuracy: {acc_targeted_before:.4f}\")\n",
        "\n",
        "    # Apply SecureML-Guard\n",
        "    model, X_train_clean, y_train_clean, dr, fpr, overhead = secureml_guard(\n",
        "        model, X_train, y_train, X_ref, y_ref, X_val, y_val, X_test, y_test, X_target, y_target\n",
        "    )\n",
        "\n",
        "    # Evaluate After SecureML-Guard\n",
        "    acc_general_after = evaluate_model(model, X_test, y_test)\n",
        "    acc_targeted_after = evaluate_model(model, X_target, y_target)\n",
        "    print(f\"After SecureML-Guard - General Accuracy: {acc_general_after:.4f}, Targeted Accuracy: {acc_targeted_after:.4f}\")\n",
        "    print(f\"Detection Rate: {dr:.4f}, False Positive Rate: {fpr:.4f}, Overhead: {overhead:.2f}s\")\n",
        "\n",
        "    # Deployment Example (Simulated New Data)\n",
        "    X_new = X_test[:10]\n",
        "    predictions = deploy_model(model, X_new)\n",
        "    print(f\"Deployment Predictions: {predictions}\")  # Map to DOBOT arm actions\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAjU4L22mOKs",
        "outputId": "8683ac07-052f-4b6e-dc1f-80bb7e536b0e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before SecureML-Guard - General Accuracy: 0.8000, Targeted Accuracy: 0.0000\n",
            "After SecureML-Guard - General Accuracy: 0.8000, Targeted Accuracy: 0.0000\n",
            "Detection Rate: 0.0000, False Positive Rate: 0.0000, Overhead: 0.00s\n",
            "Deployment Predictions: [0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    }
  ]
}